---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  orphan: true
---

# Fitting models with different cost functions

This is the notebook from the Thursday morning class.

```{python}
import numpy as np
np.set_printoptions(suppress=True)
from scipy.optimize import minimize
import pandas as pd
pd.set_option('mode.copy_on_write', True)
import statsmodels.formula.api as smf
import sklearn.linear_model as sklm
import sklearn.metrics as skmetrics
```

```{python}
df = pd.read_csv('data/rate_my_course.csv')
#MB To make it easier to run Statsmodels, in particular.
df = df.rename(columns={'Overall Quality': 'Quality'})
df
```

Fetch some columns of interest:

```{python}
# This will be our y (the variable we predict).
helpfulness = df['Helpfulness']
# One of both of these will be our X (the predictors).
clarity_easiness = df[['Clarity', 'Easiness']]
```

Fit the model with Statsmodels.

```{python}
model = smf.ols('Helpfulness ~ Clarity + Easiness', data=df)
sm_fit = model.fit()
sm_fit.summary()
```

Fit the same model with Scikit-learn.

```{python}
sk_model = sklm.LinearRegression()
sk_fit = sk_model.fit(clarity_easiness, helpfulness)
sk_fit
```

The coefficients (the slopes for the regressors):

```{python}
sk_fit.coef_
```

The intercept:

```{python}
sk_fit.intercept_
```

Compare the parameters to Statsmodels:


```{python}
sm_fit.params
```

## The fitted values and Scikit-learn

The values predicted by the (Sklearn) model:

```{python}
y_hat = sk_fit.predict(clarity_easiness)
y_hat
```

Compare to Statsmodels:

```{python}
sm_y_hat = sm_fit.predict()
sm_y_hat
```

```{python}
assert np.allclose(y_hat, sm_y_hat)
```

We assemble Sklearn's coefficients and intercept into a single list, with the
intercept last.

```{python}
params = list(sk_fit.coef_) + [sk_fit.intercept_]
params
```

If we just want all but the last parameter (all the coefficients, but not the intercept:

```{python}
params[:-1]
```

We could also get the parameters from Statsmodels, but we'd have to rearrange them, because Statsmodels puts the intercept first rather than last:

```{python}
sm_fit.params.iloc[[1, 2, 0]]
```

For convenience of variable names, we rename our regressors to `X`, and get
the number of rows `n` and number of regressors `p`.


```{python}
X = clarity_easiness
X = np.array(X)
n, p = X.shape
```

We then discussed how Sklearn and Statsmodels get their coefficients / parameters.   I was explaining that we can get them by direct calculation, that derives from calculus, in this simple "ordinary least squares" case.

```{python}
design = np.ones((n, 3))  # The design matrix.
design[:, :2] = X  # Put the regressors into the design
B = np.linalg.pinv(design) @ helpfulness
B
```

Notice the parameters are the same as we saw above.

We can extend this to any number of regressors:

```{python}
design = np.ones((n, 20))  # The design matrix.
design[:, :19] = np.random.normal(size=(n, 19))
B = np.linalg.pinv(design) @ helpfulness
B
```

Write a function to compute the fitted values given the parameters:


```{python}
def calc_fitted(params, X):
    """ Calculate fitted values from design X and parameters

    Parameters
    ----------
    params : vector (1D array)
        Vector of parameters, intercept is last parameter.
    X : array
        2D array with regressor columns.

    Returns
    -------
    y_hat : vector
        Vector of fitted values
    """
    X = np.array(X)
    n, p = X.shape
    y_hat = np.zeros(n)
    for col_no, param in enumerate(params[:-1]):
        y_hat = y_hat + param * X[:, col_no]
    y_hat = y_hat + params[-1]
    return y_hat
```

Show that we get the same fitted values from our function as we got from the `predict` method of Sklearn:

```{python}
my_y_hat = calc_fitted(params, clarity_easiness)
assert np.allclose(my_y_hat, y_hat)
```

Here I was showing you how `enumerate` works:

```{python}
for col_no in range(2):
    print(col_no)
```

```{python}
for param in params[:-1]:
    print(param)
```

```{python}
for col_no, param in enumerate(params[:-1]):
    print(col_no, param)
```

Calculate the error vector and then calculate the sum of squared error:

```{python}
e = helpfulness - calc_fitted(params, X)
np.sum(e ** 2)
```

Make a function to calculate sum of squared error:

```{python}
def sos(params, X, y):
    y_hat = calc_fitted(params, X)
    e = y - y_hat  # residuals
    return np.sum(e ** 2)
```

Check that we get the same answer from the function as we got from calculating above:

```{python}
sos(params, clarity_easiness, helpfulness)
```

Use `minimize` to find parameters minimizing the sum of squared error:

```{python}
min_res = minimize(sos, [0, 0, 0], args=(clarity_easiness, helpfulness))
min_res
```

Yes, the parameters (coefficients and intercept) are the same as we got from Statsmodels and Sklearn:

```{python}
min_res.x
```

## The long and the short of $R^2$

Sklearn has an `r2_score` metric.

```{python}
skmetrics.r2_score(helpfulness, y_hat)
```

We can also see the $R^2$ metric in the Statsmodels output.

```{python}
sm_fit.summary()
```

We already know the formula for R^2^.  We can calculate by hand to show this
gives the same answer.

For notational convenient, give our `y` vector the variable name `y`:

```{python}
y = helpfulness
```

Sum of squares around mean:

```{python}
ss_mean = np.sum((y - np.mean(y)) ** 2)
ss_mean
```

Sum of squares from full model:

```{python}
ss_model = np.sum(e ** 2)
ss_model
```

Our by-hand calculation gives the same answer as Sklearn or Statsmodels:

```{python}
1 - ss_model / ss_mean
```
